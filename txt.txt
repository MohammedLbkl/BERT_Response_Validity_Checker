Introdution :

Le but du stage est de développer un système d'apprentissage de l'anglais 
dans un environnement interactif.
Il se compose de plusieurs étapes qui peuvent être réalisées de manière 
indépendante comme comparer des modeles de transciption effectuer une classification des réponse cohérante de l'utilisateur detecter les erreur de prononciation ou bien utiliser des llm pour proposer un feedback des erreur apres chaque exercices.

Alors comment fonctionne exactement l'application,
L'application simule des scènes proches de la réalité comme acheter des 
articles dans un super marcher, des interactions sont fait tout au long
de l'exercice. Il faut savoir lorsqu'on doit répondre à une question un 
mots est donné et le but est de répondre de manière cohérante en 
utilisant ce mot. 
Et d'ailleur on possede aussi les scriptes de chaque scene en avance



1.Comparaison des modeles de transcription comme vosk et whisper
(Definir Whisper) : Calculer le WER(Definir le WER) et la vitesse de 
transcription (Important)

-Whisper est un Transformer de type Encoder-Decoder:

1. L'Encodeur :

.Input : Des segments audio de 30 secondes, convertis en spectrogrammes log-mel(une représentation visuelle du spectre de fréquences d'un signal audio, mais qui a été modifiée pour imiter la perception humaine de l'ouïe.) .

.Front-end Convolutionnel : Avant les blocs Transformer, l'input passe par deux couches convolutionnelles (kernel 3, stride 1 & 2).
Rôle : C'est une pratique standard pour réduire la longueur de la séquence avant de l'envoyer à l'attention quadratique des Transformers, tout en capturant des motifs acoustiques locaux (phonèmes, transitions).

.Blocs Transformer de l'Encodeur : Une pile standard de blocs Transformer avec auto-attention et couches feed-forward. Leur rôle est de construire des représentations contextuelles riches de l'intégralité du segment audio de 30 secondes.

2. Le Décodeur:

.Architecture : Un Transformer Décodeur standard qui prédit le prochain token de texte en se basant sur les tokens précédents et les représentations de l'encodeur (via l'attention croisée).

. : Whisper n'est pas seulemnt capable de transcrire des phrases il est aussi capable de prédire la langue de l'utilisateur et d'effectuer des tache de traduction. 

3. L'Attention Croisée (Cross-Attention) : Le Pont
C'est le mécanisme clé qui permet au décodeur, à chaque étape de génération, de "regarder" les parties les plus pertinentes de la sortie de l'encodeur audio pour prédire le token de texte suivant.


- Vosk : 

.Contrairement à Whisper, qui a été conçu avec une approche "cloud-first" (modèles massifs, généralisation extrême), Vosk est optimisé pour des contraintes opposées : faible latence, faible consommation de ressources (RAM/CPU), et fonctionnement hors-ligne. Chaque choix architectural découle de cette contrainte fondamentale.

.Rôle : Estimer la probabilité d'une séquence de mots. P(mot_N | mot_1, ..., mot_{N-1}).

.Pourquoi pas un Transformer/RNN ? Un LM neuronal est lourd, lent et gourmand en mémoire. Un modèle n-gram, une fois compilé, est une structure de données extrêmement rapide à requêter, ce qui est crucial pour la faible latence. Il est stocké sous forme de FST (Finite State Transducer).


2.On utiliser l'encodeur de Whisper pour transformer les audios en spectrogrammes log-mel pour permetre à un 
modele de machine learning simple comme une régression
logistique de comprendre et ainsi faire une prediction 
d'accent(Pas Important)

Objectif : Construire un pipeline complet pour classifier des accents (ex: US, UK, Australien, Indien) à partir de spectrogrammes log-mel, en utilisant des modèles de ML traditionnels.

.Spectrogrammes log-mel: Un spectrogramme log-mel est une représentation visuelle du spectre de fréquences d'un signal audio, mais qui a été modifiée pour imiter la perception humaine de l'ouïe.
C'est une transformation en 3 étapes d'un signal audio brut (une onde) vers une "image" que le réseau de neurones peut interpréter.
Signal audio brut (onde) → Spectrogramme → Spectrogramme Mel → Spectrogramme Log-Mel
-Étape 1 : Du Signal au Spectrogramme (via la STFT)
Un signal audio est une série de valeurs d'amplitude dans le temps. Ce n'est pas très informatif pour un modèle. Nous voulons savoir quelles fréquences sont présentes à chaque instant.
Le Problème : Une simple Transformée de Fourier (FFT) nous donnerait le contenu fréquentiel de tout le fichier, perdant toute information temporelle. Nous ne saurions pas quand une fréquence est apparue.
La Solution : La Transformée de Fourier à Court Terme (Short-Time Fourier Transform - STFT)
On découpe le signal en petites fenêtres temporelles qui se chevauchent (ex: des fenêtres de 25 millisecondes).
On applique une Transformée de Fourier sur chaque fenêtre.
On empile les résultats.

Le résultat est un spectrogramme : une matrice 2D où :
L'axe X représente le temps (chaque colonne est une fenêtre).
L'axe Y représente la fréquence (sur une échelle linéaire, ex: de 0 Hz à 8000 Hz).
L'intensité (la couleur ou la valeur dans la matrice) représente l'amplitude (l'énergie) de chaque fréquence à chaque instant.


-Étape 2 : De l'Échelle Linéaire à l'Échelle Mel (Le "Mel" de Log-Mel)
C'est l'étape cruciale. L'oreille humaine ne perçoit pas les fréquences de manière linéaire.
Basses Fréquences (ex: 100 Hz vs 200 Hz) : Nous sommes très sensibles aux variations. La différence est énorme pour nous.
Hautes Fréquences (ex: 7000 Hz vs 7100 Hz) : Nous peinons à distinguer cette différence de 100 Hz. Elle est beaucoup moins pertinente.
L'échelle de Mel est une échelle perceptive conçue pour imiter cette non-linéarité. Elle accorde plus de résolution aux basses fréquences (cruciales pour distinguer les voyelles et les fondamentales de la voix) et moins aux hautes fréquences.


On utilise un banc de filtres Mel (Mel filterbank). C'est un ensemble de filtres triangulaires qui sont :
Étroits et rapprochés dans les basses fréquences.
Larges et espacés dans les hautes fréquences.
Un banc de filtres Mel : les filtres sont plus fins et nombreux en basses fréquences.
On applique ce banc de filtres au spectrogramme linéaire de l'étape 1. Chaque filtre triangulaire va "agréger" l'énergie des fréquences qu'il recouvre.

Le résultat est un Spectrogramme Mel. L'axe Y n'est plus la fréquence linéaire, mais le numéro du bin Mel. Par exemple, Whisper utilise 80 bins Mel, ce qui signifie que tout le spectre de fréquences est résumé en 80 valeurs pondérées perceptivement.
Pourquoi c'est décisif ?
Réduction de dimensionnalité : On passe de (par exemple) 1024 points de fréquence de la FFT à seulement 80 ou 128 bins Mel. C'est beaucoup plus efficace pour le modèle.
Pertinence Biologique : On donne au modèle une représentation de l'audio qui est plus proche de ce que notre propre système auditif traite. Les informations les plus importantes pour la compréhension de la parole sont mises en avant.

-Étape 3 : De l'Amplitude à l'Échelle Logarithmique (Le "Log" de Log-Mel)
L'amplitude du son (le volume) est également perçue de manière non-linéaire par les humains. Nous sommes beaucoup plus sensibles aux variations de sons faibles qu'aux variations de sons forts. C'est une perception logarithmique, mesurée en décibels (dB).
Pour mimer cela, on prend simplement le logarithme de l'amplitude (de l'énergie) de chaque point du spectrogramme Mel.

Pertinence Biologique : Encore une fois, cela rapproche la représentation de la perception humaine. Un doublement de l'énergie sonore ne sera pas perçu comme deux fois plus fort.


Le résultat est une "image" dense en information, où les caractéristiques les plus pertinentes pour la reconnaissance de la parole humaine sont structurellement mises en évidence



Module 1 : De l'Image à la Donnée Structurée - Le Problème Fondamenta

Comme un spectrogramme log-mel est une image (une matrice 2D). Les modèles comme Random Forest ou XGBoost ne peuvent pas prendre une image en entrée directe. Ils attendent un vecteur de caractéristiques (features) 1D, où chaque colonne a une signification précise.

Notre premier défi, et le plus critique, est donc la réduction de dimensionnalité intelligente, aussi appelée feature engineering. Aplatir simplement la matrice du spectrogramme (ex: 80 bins x 150 pas de temps = 12 000 features) serait une catastrophe :
Malédiction de la dimensionnalité (Curse of Dimensionality) : Trop de features pour le nombre d'échantillons, menant à un sur-apprentissage massif.
Perte de l'information spatiale/temporelle : Les relations entre pixels adjacents seraient détruites.

Source de Données : Utiliser un corpus labellisé comme Common Voice (Mozilla) ou L2-Arctic, qui contiennent des métadonnées sur l'accent des locuteurs.
Génération des Spectrogrammes Log-Mel : Pour chaque segment, générer le spectrogramme log-mel grace a l'Encodeur de Whisper. On obtient une collection de matrices 2D, chacune associée à une étiquette d'accent.

Module 2 : l'Extraction de Caractéristiques (Feature Engineering)


Approche 1 : Caractéristiques Statistiques Globales

Pour chaque spectrogramme (chaque matrice 2D), calculez un ensemble de statistiques agrégées :
Moyenne et Écart-type de tous les pixels : Donne une idée de la distribution générale de l'énergie.
Skewness et Kurtosis : Mesurent l'asymétrie et "l'épaisseur" de la queue de la distribution d'énergie.
Moyenne et Écart-type par bin Mel : Calculez la moyenne et l'écart-type pour chaque ligne (chaque bin Mel) du spectrogramme. Si vous avez 80 bins, cela vous donne déjà 160 features très informatives sur la "signature spectrale" moyenne de l'accent.

Approche 2 : Les MFCC (Mel-Frequency Cepstral Coefficients)

Les MFCC sont le "gold standard" de la feature engineering en traitement de la parole.
Qu'est-ce que c'est ? Pour un expert, la meilleure définition est : le résultat d'une Transformée en Cosinus Discrète (DCT) appliquée au spectre de puissance log-mel.
Pourquoi c'est puissant ? La DCT décorrèle les énergies des bins Mel (qui sont très corrélées entre elles). Les premiers coefficients de la DCT (généralement 13 à 20) capturent l'essentiel de la forme du spectre (l'enveloppe spectrale), qui est directement liée au timbre de la voix et à l'articulation des voyelles (les formants).
Comment l'utiliser ici ? Pour chaque fenêtre temporelle (chaque colonne du spectrogramme), calculez les 13 premiers MFCC. Vous obtenez une nouvelle matrice (13 MFCC x 150 pas de temps). Ensuite, calculez la moyenne et l'écart-type de chaque coefficient MFCC sur toute la durée du segment. Cela vous donne 13 (moyennes) + 13 (écarts-types) = 26 features extrêmement puissantes par segment audio.
Votre "X" Final :
À la fin de ce module, pour chaque segment audio, vous avez un seul vecteur 1D. Par exemple : [mean_mel_1, std_mel_1, ..., mean_mel_80, std_mel_80, mean_mfcc_1, std_mfcc_1, ..., mean_mfcc_13, std_mfcc_13]. C'est sur ce tableau de données structurées que nous allons entraîner nos modèles.

Module 3 : Évaluation et Limites de l'Approche
Métrique d'Évaluation : Ne vous contentez pas de l'accuracy. La matrice de confusion est votre meilleur outil. Elle vous montrera exactement quelles paires d'accents sont les plus difficiles à distinguer (ex: US vs Canadien, Australien vs Néo-Zélandais). Calculez aussi le F1-score par classe pour gérer les déséquilibres potentiels.

Perspective Moderne : La Voie du Deep Learning (CNN)

L'approche moderne consiste à traiter le spectrogramme log-mel comme ce qu'il est : une image.

Le Modèle : Un Réseau de Neurones Convolutif (CNN).
Le Principe : Au lieu de définir manuellement des features, les filtres convolutifs du CNN vont apprendre eux-mêmes à détecter les motifs pertinents directement dans l'image du spectrogramme.
Les premières couches apprendront des motifs simples : des bords, des lignes verticales (plosives), des lignes horizontales (harmoniques).
Les couches plus profondes apprendront à combiner ces motifs pour détecter des formes plus complexes : des glissements de formants caractéristiques d'une diphtongue, des signatures spectrales de certaines voyelles spécifiques à un accent.
Avantage : Le modèle apprend les features optimales à partir des données, capturant des informations temporelles et spectrales que notre feature engineering manuel aurait manquées. C'est pourquoi cette approche donne des résultats nettement supérieurs.

Module 3.5 (Intercalaire) : La Gestion du Déséquilibre des Classes - De la Théorie à la Pratique

1. Pourquoi le Déséquilibre est-il un Problème Critique ?

Les algorithmes de Machine Learning sont, par défaut, optimisés pour minimiser une fonction de perte globale (comme l'erreur quadratique ou la cross-entropy).
Le Piège de l'Accuracy : Si 90% de vos données sont l'accent "US", un modèle "paresseux" peut atteindre 90% de précision simplement en prédisant "US" pour chaque échantillon. Du point de vue de la fonction de perte, c'est une excellente solution. Du point de vue de l'utilité, c'est un échec total.
Le Biais d'Apprentissage : Le modèle va consacrer la majeure partie de sa "capacité d'apprentissage" à perfectionner la frontière de décision autour de la classe majoritaire, car c'est là qu'il peut obtenir le plus grand gain de performance globale. Il va effectivement traiter les classes minoritaires comme du "bruit" ou des "outliers" et ne jamais apprendre à les distinguer correctement.

Pondération des Classes (Class Weighting) :
Comment ça marche : Vous assignez un poids plus élevé aux classes minoritaires dans la fonction de perte. Une erreur de classification sur un échantillon d'une classe minoritaire coûtera beaucoup plus cher au modèle qu'une erreur sur une classe majoritaire. Cela "force" le modèle à y prêter attention.

Utilisation : Le plus simple est de mettre class_weight='balanced'. L'algorithme calculera automatiquement les poids inverses à la fréquence des classes.
Pourquoi c'est excellent : Pas de modification des données, pas de risque de fuite de données, simple à mettre en œuvre et souvent très efficace.






3.Créé un modele capable de déterminer si une réponse est cohérent 
avec la question posé dans un theme précis : (Important)

Pour l'instant l'application fonction avec un systeme de mots clés 
en utilisant un NER (Definir NER) comme Spacy, si le mot clé est reconnu 
par le systeme alors la réponse donné sera correcte donc il est 
tres simple de contourné le systeme,
Après avoir testé différentes méthodes, nous avons décidé de faire 
une approche de classification binaire en faisait un fine-tuning 
de BERT (definir BERT),
le fine-tuning consiste à adapter un modele pré entrainé à une tâche 
spécifique, cette méthode permet de bénéficier des connaissances 
déjà acquise par le modèle lors de son entraînement initial tout en 
le spécialisant pour notre problème.

.L'Attention est Tout ce dont Vous avez Besoin (Vraiment) : Contrairement aux architectures séquentielles (RNN/LSTM), le mécanisme de self-attention du Transformer permet à chaque token de la séquence d'interagir directement avec tous les autres tokens. Dans notre cas, cela signifie que chaque mot de la Question peut "regarder" chaque mot de la Réponse (et vice-versa) simultanément, capturant ainsi des dépendances complexes et des relations sémantiques fines, ce qui est impossible avec un simple état caché récurrent.

.Bidirectionnalité et Pré-entraînement :
Masked Language Model (MLM) : Le pré-entraînement sur MLM force le modèle à comprendre le contexte gauche et droit pour prédire un token masqué. Cela lui confère une compréhension sémantique profonde et robuste, bien au-delà des simples cooccurrences.
Next Sentence Prediction (NSP) : C'est la tâche de pré-entraînement la plus pertinente pour notre cas d'usage. BERT a été entraîné à prédire si deux phrases A et B sont consécutives dans un corpus. Bien que l'efficacité de NSP soit débattue (et abandonnée par des modèles comme RoBERTa), l'architecture sous-jacente pour gérer des paires de phrases est la clé de notre succès.

3. Le Processus de Fine-Tuning
Préparation des données : On possede deja les scriptes de differente scene il suffit alors de générer les réponses pour optenir notre jeu de donné
Dataset : Une liste de triplets (question, réponse, label).
Tokenization : Utiliser le BertTokenizer correspondant au modèle pré-entraîné. Le tokenizer gère l'ajout des tokens spéciaux ([CLS], [SEP]), la conversion en IDs, la création de l'attention mask et des token type IDs.
Optimiseur & Scheduler :
AdamW : C'est l'optimiseur de choix, pas Adam. AdamW découple la L2 regularization (weight decay) de la mise à jour du gradient, ce qui s'est avéré plus stable et efficace pour les Transformers.
Learning Rate Scheduler : Un taux d'apprentissage constant est sous-optimal. La pratique standard est un scheduler linéaire avec une phase de warmup. Le learning rate augmente linéairement sur un petit nombre de pas initiaux (warmup) puis décroît linéairement jusqu'à 0. Cela aide à stabiliser l'entraînement au début, lorsque le modèle s'adapte à la nouvelle tâche. Learning rate typique : 2e-5 à 5e-5.


4.Optimisation Avancée avec la Focal Loss pour Maximiser le Recall

La fonction de coût standard, la CrossEntropyLoss, traite toutes les erreurs de la même manière. Cependant, dans notre cas, une erreur est plus coûteuse qu'une autre : un Faux Négatif (classer une paire Pertinente comme Non Pertinente) est pire pour nous si notre objectif est de ne rater aucune bonne réponse (maximiser le recall).


2. La Mécanique de la Focal Loss

La Focal Loss est une modification de la Cross-Entropy. Partons de la formule de la Cross-Entropy (CE) pour une prédiction binaire :

CE(p_t) = -log(p_t)
où p_t est la probabilité prédite par le modèle pour la classe correcte.
La Focal Loss y ajoute deux paramètres clés, alpha (α) et gamma (γ) :
FL(p_t) = -α_t * (1 - p_t)^γ * log(p_t)

-alpha (α) : Le Poids de Classe Statique (Le "poid" que vous mentionnez)
C'est un facteur de pondération direct pour chaque classe. Il combat le déséquilibre de classes.
Vous pouvez le définir pour donner plus d'importance à la classe minoritaire. Si votre classe Pertinent ne représente que 10% des données, vous pourriez définir α_pertinent = 0.9 et α_non_pertinent = 0.1.
En termes d'implémentation, α_t est le α correspondant à la classe de l'exemple traité.
Votre objectif : Pour maximiser le recall de la classe Pertinent, vous allez assigner un α significativement plus élevé à cette classe.


-gamma (γ) : Le Facteur de Focalisation Dynamique
C'est l'innovation majeure. Le terme (1 - p_t)^γ est le modulating factor.
Cas 1 : Exemple Facile. Si le modèle est très confiant et correct (p_t -> 1), le facteur (1 - p_t) devient très petit (-> 0). La loss pour cet exemple est donc massivement réduite. Le modèle "ignore" cet exemple facile.
Cas 2 : Exemple Difficile. Si le modèle se trompe ou est incertain (p_t -> 0), le facteur (1 - p_t) reste proche de 1. La loss n'est pas réduite et le modèle est forcé de se concentrer sur cet exemple difficile.
γ (gamma, typiquement ≥ 1, valeur commune = 2) est le paramètre de focalisation. Plus γ est grand, plus l'effet de focalisation est prononcé. Avec γ=0, on retombe sur la Cross-Entropy pondérée par alpha.


Cette double pression force le modèle à ajuster son seuil de décision pour être plus "généreux" envers la classe Pertinent, ce qui réduit les Faux Négatifs et augmente mécaniquement le recall.

Attention une configuration trop agressive de la Focal Loss peut conduire à un phénomène de "collapse" de la classification, où le modèle, pour minimiser sa perte à tout prix, adopte une stratégie simpliste mais efficace : prédire systématiquement la classe que vous avez surpondérée.



4-Créer un Agent IA de Feedback en Anglais pour une Application Audio

Objectif : Comprendre comment concevoir et implémenter un agent IA capable de fournir un retour constructif à un utilisateur après un exercice d'anglais parlé. Nous explorerons les deux architectures principales : l'utilisation d'une API externe et l'hébergement d'un modèle de langage (LLM) en local.

Analyse et Génération du Feedback (Le Cerveau IA - LLM)
Le Conversation transcrit est envoyé au LLM.
Le LLM, guidé par un prompt spécifique, analyse le texte pour y déceler des erreurs de grammaire, de syntaxe, des suggestions de vocabulaire, etc.
Il génère une réponse structurée contenant le feedback.


Option A : Utiliser une API (Ex: OpenAI, Google Gemini, Anthropic, Mistral AI)

Avantages :
Performance de Pointe : Vous accédez aux modèles les plus puissants et les plus à jour du marché (ex: GPT-4, Claude 3, Gemini Pro).
Scalabilité Facile : Que vous ayez 10 ou 10 millions d'utilisateurs, le fournisseur gère la charge.
Pas de Maintenance Matérielle : Vous n'avez pas à vous soucier des cartes graphiques (GPU), de la RAM ou du refroidissement.


Inconvénients :
Coût : Vous payez à l'usage (par "token", c'est-à-dire par mot ou partie de mot). Pour une application populaire, la facture peut vite grimper.
Latence : La requête doit faire un aller-retour sur Internet, ce qui ajoute un délai perceptible (souvent 1 à 3 secondes).
Dépendance : Vous êtes dépendant de la disponibilité et des conditions tarifaires du fournisseur. S'il tombe en panne, votre application aussi.
Connexion Internet Obligatoire : Ne fonctionne pas hors ligne.


Option B : Utiliser un LLM Local (Ex: Llama 3, Mistral 7B, Phi-3)

Vous téléchargez un modèle de langage "open source" et vous l'exécutez sur votre propre matériel. Cela peut être sur le serveur de votre application ou, pour les modèles les plus petits, directement sur l'appareil de l'utilisateur (téléphone ou ordinateur).

Avantages :
Confidentialité Totale : Les données ne quittent jamais votre infrastructure (ou l'appareil de l'utilisateur). C'est un argument de vente majeur.
Pas de Coût par Requête : Une fois le matériel acheté et configuré, vous pouvez faire autant d'appels que vous voulez sans surcoût.
Contrôle Total et Personnalisation : Vous pouvez affiner ("fine-tune") le modèle sur vos propres données pour qu'il soit parfaitement adapté à votre cas d'usage.
Fonctionnement Hors Ligne : Si le modèle tourne sur l'appareil de l'utilisateur, l'application peut fonctionner sans connexion internet.
Faible Latence : Les requêtes sont locales, donc beaucoup plus rapides (pas de latence réseau).

 Inconvénients :
Coût Matériel Initial : Pour faire tourner un bon modèle sur un serveur, il faut des GPU puissants et coûteux (plusieurs milliers d'euros).
Performance du Modèle : Les modèles open source exécutables localement sont généralement moins puissants que les géants comme GPT-4. Ils peuvent faire plus d'erreurs ou donner des feedbacks moins nuancés.
Ressources sur l'Appareil Client : Faire tourner un LLM sur un téléphone consomme beaucoup de batterie et de RAM, et nécessite de télécharger un modèle de plusieurs Gigaoctets.


Module 3 : Le "Prompt Engineering" - L'Art de Parler à l'IA
Que vous utilisiez une API ou un LLM local, la qualité de votre feedback dépendra à 80% de la qualité de votre prompt système. C'est l'instruction initiale que vous donnez à l'IA pour définir son rôle et sa tâche.


Pourquoi le JSON ? Un format structuré comme le JSON est beaucoup plus facile à manipuler dans votre code qu'un simple bloc de texte. Vous pouvez facilement extraire chaque partie pour l'afficher ou la lire différemment dans votre application.






On commence par optimiser l'approche de prompt engineering. C'est souvent suffisant et 
beaucoup plus simple. Si et seulement si cela ne donne pas de résultats assez fiables, on peut envisager le fine-tuning.


-La meilleure pratique est de structurer les instructions en utilisant un "dialogue d'exemple" (Few-shot prompting).
Pourquoi c'est mieux ?

Clarté : Le modèle voit une conversation où un utilisateur donne une phrase et où "l'assistant" (lui-même) 
répond exactement dans le format attendu. C'est la manière la plus efficace de lui apprendre un format de sortie.



--Voici pourquoi "trop d'exemples" dans le prompt est contre-productif :

-Le "Bruit" et la Dilution de l'Instruction:
En "noyant" le modèle sous 20 exemples, on dilue la puissance de 
l'instruction principale (SYSTEM_PROMPT). Le modèle peut avoir du mal à extraire le principe général 
("corrige la grammaire et la logique") et va plutôt essayer de trouver un exemple qui correspond presque parfaitement 
à la nouvelle phrase. S'il n'en trouve pas, sa réponse peut devenir imprévisible.

Analogie : C'est comme si vous appreniez à quelqu'un à cuisiner. Au lieu de lui dire "sale et poivre à ton goût", 
vous lui donnez une liste de 20 plats avec la quantité exacte de sel pour chacun. Le jour où il doit cuisiner 
un 21ème plat qui n'est pas sur la liste, il sera perdu.


-Le Problème du "Lost in the Middle" (Perdu au milieu)

Les modèles de langage, surtout les plus petits, accordent une attention inégale aux différentes parties du prompt. 

Ils se souviennent très bien :

Du début (le SYSTEM_PROMPT).
De la toute fin (les derniers exemples et la question de l'utilisateur).

Les exemples qui se trouvent au milieu de votre longue liste ont de fortes chances d'être "oubliés" 
ou de recevoir moins d'attention. Leur utilité est donc très faible.


---L'objectif du "few-shot prompting" n'est pas d'être exhaustif, mais de donner quelques 
exemples diversifiés et de haute qualité qui illustrent parfaitement vos attentes.





4.Et apres :
Développer des tests pour évaluer les compétences de chaque utilisateur 
et ainsi regrouper les utilisateurs en différents profils, 
pour ajuster automatiquement la difficultéet et les types d'exercices.








Discussion : 
En essayant plusieur modele de llm open source en moyenne le modele le plus performant prend environ 5seconde pour corriger une phrase, d'ailleur il faut savoir que pour ce type de llm il est impossible de faire passer tout le dialogue il faut le faire phrase par phrase sinon le modele pert énnormeent en efficassite le pluspar des phrase ne sont par retenu et les explication de la tache on été fortement diluer par exemple on lui demande de corriger en anglais et de founire une correction en francais, il va oublier cette partie et tout rediger en anglais ou bien en francais il est donc primodiale de treter le dialogue phrase par phrase, le probleme c'est que meme en faisant ca ne modele rester pas assez performantet en plus consomme de la RAM (voir tableau.3) ce qui est crussial pour notre application il serait alors plus judissieux d'utiliser une API


une méthode a été utiliser pour simplifier le jeu de donné au lieux de lui donné des exemples de phrase pertinante ou non, on donne au modele seulement les phrase pertinant et non pertinant qui possede le mots clé utiliser et pour le détercter on utilisera pas le modele BERT mais plutot spacy un NER qui est utilie pour détercter les mots clés cela permetraduit de diminuer considerablement de jeu de donnés et ainsi etre plus précis dans les cas spésifique ou le mots clé et utiliser et ainsi d'iminuer considerablement le temps d'entrainement et aubtenir de bien meilleur résultat



le prochain travaux sera de crée un modele capable de dectercter les erreur de prononciation on dispose deja un jeu de donné qui a été conçu pour la détection d’erreurs de prononciation chez les locuteurs non natifs de l’anglais.Il contient environ 27 heures d’enregistrements répartis sur 24 locuteurs (12 hommes et 12 femmes) issus de différentes langues maternelles.
De plus une transcritopn de phonèmes a été effectuer
le but sera de fine-tuner un grand modèle de reconnaissance de la parole (Wav2Vec2) pour qu'il apprenne à la fois à reconnaître les phonèmes et à identifier les erreurs.

Et dans un autre temps crée un moteur de détection d'erreurs répetitives il suffira tout simplement de stocker les erreur de l'utilisateur dans une base de donné et annoter les erreur en plusieurs type pour avoir stocker kes erreurs de manière structurée pour pouvoirs les analyser

