{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042b570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle et tokenizer chargés depuis le disque et prêts pour l'inférence.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re \n",
    "\n",
    "\n",
    "MODEL_NAME_FOR_TEST = \"camembert-base\"\n",
    "MAX_LEN_FOR_TEST = 160 \n",
    "MODEL_SAVE_PATH_FOR_LOAD = \"save_model_fr/camembert_coherence_final_model.bin\"\n",
    "TOKENIZER_SAVE_PATH_FOR_LOAD = \"save_model_fr/camembert_coherence_tokenizer/\"\n",
    "DEVICE_FOR_TEST = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def clean_text_for_prediction(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\?,!àâéèêëîïôûùüçÀÂÉÈÊËÎÏÔÛÙÜÇ\\']', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "model_to_test = None\n",
    "tokenizer_to_test = None\n",
    "device_to_test = None\n",
    "max_len_to_test = None\n",
    "clean_text_func_to_use = None\n",
    "\n",
    "\n",
    "tokenizer_to_test = AutoTokenizer.from_pretrained(TOKENIZER_SAVE_PATH_FOR_LOAD)\n",
    "\n",
    "\n",
    "model_to_test = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_FOR_TEST, num_labels=2)\n",
    "\n",
    "# Charger les poids sauvegardés\n",
    "model_to_test.load_state_dict(torch.load(MODEL_SAVE_PATH_FOR_LOAD, map_location=DEVICE_FOR_TEST))\n",
    "model_to_test.to(DEVICE_FOR_TEST)\n",
    "model_to_test.eval() \n",
    "\n",
    "max_len_to_test = MAX_LEN_FOR_TEST \n",
    "clean_text_func_to_use = clean_text_for_prediction\n",
    "device_to_test = DEVICE_FOR_TEST \n",
    "\n",
    "print(\"Modèle et tokenizer chargés depuis le disque et prêts pour l'inférence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9123ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonction de prédiction (utilise les variables _to_test) ---\n",
    "def predict_coherence(question, answer, model, tokenizer, device, max_len, clean_fn):\n",
    "\n",
    "    cleaned_question = clean_fn(question)\n",
    "    cleaned_answer = clean_fn(answer)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        cleaned_question,\n",
    "        cleaned_answer,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        prediction_idx = torch.argmax(probs, dim=1).cpu().item()\n",
    "        probability_score = probs[0][prediction_idx].cpu().item()\n",
    "\n",
    "    return prediction_idx, probability_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e84f9150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple 1:\n",
      "  Question: \"où puis-je trouver le lait ?\"\n",
      "  Réponse:  \"dans le rayons des oeufs.\"\n",
      "  Prédiction: Non Cohérent (Label: 0, Confiance: 0.7411)\n",
      "\n",
      "Exemple 2:\n",
      "  Question: \"tu manges du poulet ?\"\n",
      "  Réponse:  \"le poulet est un menbre de ma famille\"\n",
      "  Prédiction: Non Cohérent (Label: 0, Confiance: 0.9247)\n",
      "\n",
      "Exemple 3:\n",
      "  Question: \"Je cherche du café moulu\"\n",
      "  Réponse:  \"Le ciel est bleu et les oiseaux chantent.\"\n",
      "  Prédiction: Non Cohérent (Label: 0, Confiance: 0.9231)\n",
      "\n",
      "Exemple 4:\n",
      "  Question: \"Où sont les tomates en conserve ?\"\n",
      "  Réponse:  \"Avez-vous pensé à regarder la météo ?\"\n",
      "  Prédiction: Non Cohérent (Label: 0, Confiance: 0.9210)\n",
      "\n",
      "Exemple 5:\n",
      "  Question: \"Quel est le prix des pommes ?\"\n",
      "  Réponse:  \"Elles sont à 2.50€ le kilo cette semaine.\"\n",
      "  Prédiction: Cohérent (Label: 1, Confiance: 0.9148)\n",
      "\n",
      "Exemple 6:\n",
      "  Question: \"Avez-vous du pain complet ?\"\n",
      "  Réponse:  \"Non, désolé, nous n'avons plus de pain complet aujourd'hui.\"\n",
      "  Prédiction: Cohérent (Label: 1, Confiance: 0.9214)\n",
      "\n",
      "Exemple 7:\n",
      "  Question: \"Je voudrais des piles AA.\"\n",
      "  Réponse:  \"Les piles se trouvent généralement près des caisses ou au rayon électronique.\"\n",
      "  Prédiction: Cohérent (Label: 1, Confiance: 0.9131)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Exemples de test ---\n",
    "test_samples = [\n",
    "    {\"question\": \"où puis-je trouver le lait ?\", \"answer\": \"dans le rayons des oeufs.\"},\n",
    "    {\"question\": \"tu manges du poulet ?\", \"answer\": \"le poulet est un menbre de ma famille\"},\n",
    "    {\"question\": \"Je cherche du café moulu\", \"answer\": \"Le ciel est bleu et les oiseaux chantent.\"}, # Incohérent\n",
    "    {\"question\": \"Où sont les tomates en conserve ?\", \"answer\": \"Avez-vous pensé à regarder la météo ?\"}, # Incohérent\n",
    "    {\"question\": \"Quel est le prix des pommes ?\", \"answer\": \"Elles sont à 2.50€ le kilo cette semaine.\"},\n",
    "    {\"question\": \"Avez-vous du pain complet ?\", \"answer\": \"Non, désolé, nous n'avons plus de pain complet aujourd'hui.\"},\n",
    "    {\"question\": \"Je voudrais des piles AA.\", \"answer\": \"Les piles se trouvent généralement près des caisses ou au rayon électronique.\"}\n",
    "]\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    q = sample[\"question\"]\n",
    "    a = sample[\"answer\"]\n",
    "\n",
    "    print(f\"\\nExemple {i+1}:\")\n",
    "    print(f\"  Question: \\\"{q}\\\"\")\n",
    "    print(f\"  Réponse:  \\\"{a}\\\"\")\n",
    "\n",
    "    pred_label, pred_prob = predict_coherence(q, a,\n",
    "                                                model_to_test,\n",
    "                                                tokenizer_to_test,\n",
    "                                                device_to_test,\n",
    "                                                max_len_to_test,\n",
    "                                                clean_text_func_to_use)\n",
    "\n",
    "\n",
    "    coherence_status = \"Cohérent\" if pred_label == 1 else \"Non Cohérent\"\n",
    "    print(f\"  Prédiction: {coherence_status} (Label: {pred_label}, Confiance: {pred_prob:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
