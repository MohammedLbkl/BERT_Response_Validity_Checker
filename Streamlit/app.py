import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import re

# --- Constantes (adapt√©es de ton script) ---
MODEL_NAME_FOR_APP = "camembert-base"
MAX_LEN_FOR_APP = 160
MODEL_SAVE_PATH_FOR_APP = "../notebooks/BERT_fr/save_model_fr/camembert_coherence_final_model.bin"
TOKENIZER_SAVE_PATH_FOR_APP = "../notebooks/BERT_fr/save_model_fr/camembert_coherence_tokenizer/"
DEVICE_FOR_APP = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- Fonction de nettoyage de texte (identique √† ton script) ---
def clean_text_for_prediction(text):
    text = str(text)
    text = re.sub(r'<.*?>', '', text) # Enlever les balises HTML
    text = re.sub(r'[^a-zA-Z0-9\s\.\?,!√†√¢√©√®√™√´√Æ√Ø√¥√ª√π√º√ß√Ä√Ç√â√à√ä√ã√é√è√î√õ√ô√ú√á\']', '', text) # Garder seulement les caract√®res pertinents
    text = text.lower() # Mettre en minuscule
    text = re.sub(r'\s+', ' ', text).strip() # Normaliser les espaces
    return text

# --- Fonction pour charger le mod√®le et le tokenizer (mise en cache) ---
 # Important pour ne charger le mod√®le qu'une fois
@st.cache_resource 
def load_model_and_tokenizer():
    try:
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SAVE_PATH_FOR_APP)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_FOR_APP, num_labels=2)
        model.load_state_dict(torch.load(MODEL_SAVE_PATH_FOR_APP, map_location=DEVICE_FOR_APP))
        model.to(DEVICE_FOR_APP)
        model.eval()
        st.sidebar.success("Mod√®le et tokenizer charg√©s avec succ√®s!")
        return model, tokenizer
    except Exception as e:
        st.sidebar.error(f"Erreur lors du chargement du mod√®le/tokenizer: {e}")
        st.sidebar.error(f"V√©rifiez que les chemins existent :")
        st.sidebar.error(f"Mod√®le: {MODEL_SAVE_PATH_FOR_APP}")
        st.sidebar.error(f"Tokenizer: {TOKENIZER_SAVE_PATH_FOR_APP}")
        return None, None

# --- Fonction de pr√©diction (identique √† ton script, mais utilise les variables de l'app) ---
def predict_coherence(question, answer, model, tokenizer, device, max_len, clean_fn):
    cleaned_question = clean_fn(question)
    cleaned_answer = clean_fn(answer)

    encoding = tokenizer.encode_plus(
        cleaned_question,
        cleaned_answer,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False, # Important pour Camembert/RoBERTa
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt',
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=1)
        prediction_idx = torch.argmax(probs, dim=1).cpu().item()
        probability_score = probs[0][prediction_idx].cpu().item()

    return prediction_idx, probability_score

# --- Interface Streamlit ---
st.set_page_config(page_title="Coh√©rence Q-R", layout="wide")
st.title("Pr√©dicteur de Coh√©rence Question-R√©ponse")
st.markdown("""
Bienvenue ! Cette application utilise un mod√®le Camembert fine-tun√© pour pr√©dire
si une r√©ponse donn√©e est coh√©rente par rapport √† une question pos√©e.
""")

# Charger le mod√®le et le tokenizer
model, tokenizer = load_model_and_tokenizer()

if model and tokenizer:
    st.header("Entrez votre question et r√©ponse ici :")

    col1, col2 = st.columns(2)
    with col1:
        question_input = st.text_area("Question :", height=150, placeholder="Ex: O√π sont les toilettes ?")
    with col2:
        answer_input = st.text_area("R√©ponse :", height=150, placeholder="Ex: Au fond du couloir √† droite.")

    if st.button("üîç Pr√©dire la Coh√©rence", type="primary", use_container_width=True):
        if question_input and answer_input:
            with st.spinner("Pr√©diction en cours..."):
                pred_label, pred_prob = predict_coherence(
                    question_input,
                    answer_input,
                    model,
                    tokenizer,
                    DEVICE_FOR_APP,
                    MAX_LEN_FOR_APP,
                    clean_text_for_prediction
                )

            st.subheader("R√©sultat de la Pr√©diction :")
            coherence_status_text = "‚úÖ Coh√©rent" if pred_label == 1 else "‚ùå Non Coh√©rent"
            confidence_percentage = pred_prob * 100

            if pred_label == 1:
                st.success(f"**Statut :** {coherence_status_text}")
            else:
                st.error(f"**Statut :** {coherence_status_text}")

            st.metric(label="Score de Confiance", value=f"{confidence_percentage:.2f}%")

        else:
            st.warning("Veuillez entrer une question ET une r√©ponse pour la pr√©diction.")
else:
    st.error("Le mod√®le n'a pas pu √™tre charg√©. L'application ne peut pas fonctionner. V√©rifiez les logs dans la console et les chemins des fichiers.")

# Informations sur le mod√®le dans la sidebar
st.sidebar.header(" Informations sur le Mod√®le")
st.sidebar.markdown(f"**Mod√®le de base :** `{MODEL_NAME_FOR_APP}`")
st.sidebar.markdown(f"**Longueur Max. S√©quence :** `{MAX_LEN_FOR_APP}`")
st.sidebar.markdown(f"**Dispositif :** `{DEVICE_FOR_APP}`")
st.sidebar.markdown(f"**Chemin Mod√®le :** `{MODEL_SAVE_PATH_FOR_APP}`")
st.sidebar.markdown(f"**Chemin Tokenizer :** `{TOKENIZER_SAVE_PATH_FOR_APP}`")

st.sidebar.markdown("---")
st.sidebar.markdown("Application cr√©√©e avec Streamlit.")